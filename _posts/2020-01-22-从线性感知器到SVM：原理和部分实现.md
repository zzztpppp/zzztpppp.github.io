## 从线性感知器到SVM：原理和（部分）实现

### 简介

线性感知器（linear perceptron）模型是一个实现和原理都相对简单的二元线性分类器，在它基础上衍生而来的线性支持向量分类器（Linear Support Vector Classifier）拥有更高的稳定性；而在LinearSVC的基础上，通过核函数把特征向高维映射来得到在现有特征空间里的非线性分类能力的就是支持向量机分类器。下面我们来简单地讲一讲各自的原理和实现。

### 线性感知器（linear perceptron）

线性感知器分类器的目的是找到一个平面，是得该平面能够将两类不同的数据点完全切分。它的基本方法就是探测所有的数据点，当遇到当前平面不能切分的数据时，它就相应的做出相关调整（调整切分平面的偏转度，距离原点的距离）。它的迭代过程大致如下：

1. 初始化切分平面（通常为 **0** 向量）
2. 感知：计算下一个数据点在当前平面下分类的正确性。
3. 调整：如果 2 中的数据点被错误分类，那么调整相应的切分平面，否则无动作
4. 迭代：重复步骤 2

**分类原理**：

假设我们有两类数据：$\ (\textbf{x}^{+}_{i}, y^{+}_{i})$ 和 $(\textbf{x}^{-}_{j}, y^{-}_{j})$ ，其中标签数据$\ y^{+}_{i}$ 全为 1， $\ y^{-}_{j}$ 全为 -1。

在空间中，一个平面可以由表达式$\ \theta \cdot x + \theta_{0}= 0$ 表示，我们的目的是，当数据被正确地分类时，希望：一类数据点$\ x^{+}_{i}$ 有， $\ \theta \cdot x^{+}_{i} >-\theta_{0} $ ；另一类数据点$\ x^{-}_{j}$ 有，$\ \theta \cdot x^{-}_{j} < -\theta_{0} $ 。这样，我们就找到了一个平面将两类数据完全正确的分类了。如下图展示了一个正确的二维分类的例子：

![class](/assets/svm/class.png)

**感知原理 **：

根据上面的分类原理，我们知道，如果对于一个数据点$\ \textbf{x}_{i}$，有$\ y_{i} * (\theta \cdot \textbf{x}_{i}+\theta_{0}) <= 0$ ，则说明我们感知到了一个分类错误。下图给出了一个感知到错误的示例：

![err](/assets/svm/err.png)

**调整原理**：

当出现分类错误时，我们更新$\ \theta $  为 $\ \theta_{next} \leftarrow \theta + y_{i}* \textbf{x}_{i}$ ，更新$\ \theta_{0}$为 $$。即，当应该被分在平面“上方”的数据被分在了平面下方时，我们把平面“朝着”数据点的方向偏转，同时调整平面距离原点的距离，使得该数据尽可能被分在平面“上方”；当某个数据应当出现在“下方”时，我们做相反的操作。上图的错误经过调整后，结果如下图：

![err_f](/assets/svm/err_f.png)

**伪代码**

![1585125821(1)](/assets/svm/1585125821(1).png)

**代码实现**

```python
import numpy as np

# Perceptron a single data point.
def perceptron_single_step(
        x_vector, y_label, 
        current_theta, 
        current_theta_0):
    
    next_theta = current_theta
    next_theta_0 = current_theta_0
    if (y_label * current_theta.dot(x_vector) + current_theta_0) <= 0:
        next_theta = current_theta + y_label * x_vector
        next_theta_0 = current_theta_0 + y_label
    
    return next_theta, next_theta_0

def perceptron(feature_matrix, y_labels, T):
    
    # Initialize theta and theta_0 to be 0s
    theta = np.array([0] * feature_matrix.shape[1])
    theta_0 = 0
    
    # T to control rounds of iterations through data
    for t in range(T):
        for i in range(feature_matrix.shape[0]):
            theta, theta_0 =perceptron_single_step(feature_matrix[i],
                                                   y_labels[i],
                                                   theta, theta_0)
    return theta, theta_0
```



**基本特性**

- 迭代数据的顺序不同，算法的收敛速度不同，得到的结果可能也不同（尽管这些结果都是正确的）



### **平均感知器（averaged perceptron）**

在上文介绍的线性感知器算法中，无论我们的超平面在之前的数据集上做的分类有多么完美，一旦遇到了一个被错误分类的数据，它都会调整我们的超平面以尝试正确分类当前的数据点。在这种情况下，我们很可能为了纠正这一个错误而“前功尽弃”。对此，我们可以对迭代中的各个超平面做一个“投票”，即，存在时间越长的超平面，我们给它更多的权重，最后得到的超平面是迭代过程中存在过的超平面的加权平均。

**伪代码**

![1585127012](/assets/svm/1585127012.png)

**代码实现**

```python
def average_perceptron(feature_matrix, labels, T):
    
    # Initialize theta to be 0
    theta = np.array([0]*feature_matrix.shape[1])
    theta_0 = 0
    theta_sum = theta
    theta_0_sum = theta_0
    for t in range(T):
        for i in get_order(feature_matrix.shape[0]):
            theta, theta_0 = perceptron_single_step(feature_matrix[i], labels[i],
                                                           theta, theta_0)
            theta_sum = theta_sum + theta
            theta_0_sum = theta_0_sum + theta_0
    scaler = T*feature_matrix.shape[0]
    return theta_sum / scaler, theta_0_sum / scaler
```



### 从优化的角度来看问题：SVM

很多机器学习问题通常都可以转换为优化问题。比如上面线性感知问题，从优化的角度建模可以转换为最小化下面这个目标函数：
$$
J(\theta, \theta_{0}) = \frac{1} {n} \sum_{i=1}^n I(y^i(\theta \cdot x^i + \theta_0))
$$
其中$I$ 是指示函数，当输入大于0时它的值为0否则为1。可以轻易的看出，优化后得到的结果就是能够正确分类数据的参数 $\theta$ 和$\theta_0$ 。

**间隔分类器**

在上面展示的分类器中，我们通常能够找到很多个符合条件的分类器（即能够完整切分数据点的平面）。例如下图这种情况：

![multiresult](/assets/svm/multiresult.png)

间隔分类器给我们提供了在这些诸多的可能的结果中选择一个最佳分类器的准则：选择一个能最大化类边界到分类平面之间间隔距离的和的分类器。下图两虚线到分类平面的距离之和就是所谓的边界距离

![margin](/assets/svm/margin.png)

这两条分类边界的虚线可以由
$$
\theta \cdot x + \theta_0 = 1 
\\ \theta \cdot x + \theta_0 = -1
$$
来表示。则边界到分类平面的距离则为$ \frac{1}{\|\theta\|} $。因此，我们可以在不改变分类效果的情况下，通过调节$\theta$的长度来调节边界和分类平面的距离。于是，我们可以通过求解以下这个问题来得到一个最优分类器：
$$
\min_{\theta} \quad \|\theta\|\\
\textrm{s.t.}\quad \frac{1} {n} \sum_{i=1}^n I(y^i(\theta \cdot x^i + \theta_0)) = 0
$$
也就在正确分类的前提下，我们选择一个间隙距离之和最大的分类器。

**软边界**

上面的间隔分类器的边界属于硬边界：它不允许任何数据越过它。当数据出现异常点时，间隙分类器给出的结果可能就不那么合理了：

![margin_outlier](/assets/svm/margin_outlier.png)m

上图中有一个蓝类有一个异常的数据点，它的存在让我们得到地分类器可能不那么合理。我们可以通过允许部分数据跨过边界来让我们的结果更合理，如下图所示：

![margin_outlier_ignore](/assets/svm/margin_outlier_ignore.png)


硬边界造成的另一个问题就是：当我们的数据不是完全线性可分的时候（也就是有重叠），我们就找不到一个间隔分类器来切分我们的数据，比如下图这种情况：

![mix](/assets/svm/mix.png)

在解决实际问题的过程中，上图的情况相对于完全线性可分的情况应该是最常见的。为了解决上文提到的两个问题，我们通过把问题转换成下面这个问题来引入软间隔：
$$
\min_{\theta, \theta_0} \quad \frac{1}{n} \sum_{i=1}^{n}Loss_h(y^i(\theta \cdot x^i + \theta_0)) + \frac{\lambda}{2}\|\theta\|^2\\
$$
其中，$Loss_h(x)$为：
$$
Loss_h(x) = \left\{ \begin{array}{rl}
 0 &\mbox{ if $x>1$} \\
  1-x &\mbox{ otherwise}
       \end{array} \right.
$$


$\lambda$ 为调节参数，用来控制边界的宽度。我们可以理解为当$y^i(\theta \cdot x^i + \theta_0) > 1$时，这个点被正确分类了，我们不对它做任何惩罚；当$y^i(\theta \cdot x^i + \theta_0) < 1$时，它离边界越远，受到到的惩罚越大。这样我们就实现了软边界，它的分类效果如下：

![mix_result](/assets/svm/mix_result.png)

将$\lambda$调大后，效果如下：

![mix_result_large_lambda](/assets/svm/mix_result_large_lambda.png)

### 特征空间的扩展

线性支持向量分类器尝试在原始的特征空间内找到能够切分切分数据点的超平面，但是通常情况下，在原始的特征空间内我们并不能能够线性划分数据点。此时，对原始的特征空间进行扩展，可以让我们的数据在新的特征空间下变得线性可分，一个简单的例子如下图：![feature_map](/assets/svm/feature_map.png)

上左图是数据的在原始特征空间内的位置，为$(0.2), (0.4), (0.6)$，我们可以看到在原始的一维特征空间内，数据是无法被线性分割的。我们把数据扩展到二位空间，扩展后的特征分别为$(0.2, 0.5),(0.4, 0),(0.6,0.5)$那么在特征扩展之后，数据就是线性可分的了。

**特征空间扩展的线性感知器**

假设$\phi(x)$为特征扩展函数，那么在扩展的特征空间内判断一个数据是否被正确分类的条件就变成了$(y^i(\theta \cdot \phi(x^i)+\theta_0)$ ， 它的参数更新就变成了$\ \theta_{next} \leftarrow \theta + y_{i}*\phi(\textbf{x}_{i})$， $\ \theta_{0, next} \leftarrow \theta_{0} + y_{i} $

**线性核函数感知器**

其实线性感知器的参数$\theta$可以由这个公式表示
$$
\theta = \sum_{j=1}^{n}{\alpha_{j}y^j\phi(x^j)}
$$
那么，线性感知器的参数更新过程可以等价重构为以下过程
$$
if \quad \sum_{j=1}^{n}{\alpha_{j}y^j\phi(x^j)\phi(x^i)} \le 0 \\ then \quad \alpha_{i} = \alpha_{i} + 1
$$
定义$K(x^j, x^i)=\phi(x^j)\phi(x^i)$为核函数。当我们的扩展空间维度比较高时，采用具体的向量点积方式计算核函数的值相对来说比较费时，比方说我使用二次多项式来扩展特征空间，也就是$\phi(x)=[x_1, x_2, x_1^2, \sqrt2x_1x_2, x_2, x_2^2]$。那么相比直接采用扩展后的特征进行点积计算，利用公式$\phi(x)\cdot\phi(x^\prime) = x \cdot x^\prime + (x \cdot x^\prime)$ 显然效率更高，那么这种直接高效计算扩展特征的点积的函数，就是核函数。更一般的，对于$n$次多项式进行扩展的特征，用于计算两个数据之间点积的核函数为：
$$
K(x, x^\prime) = (x\cdot x^\prime + c)^d
$$
其中$c$为为可调节的参数。

**Radial basis核函数**

有了核函数能够高效计算高维的点积之后，人们甚至搞出了无限维的特征扩展。在这种情况下，我们根本不能具体的拿到在扩展空间的数据，更不用说在扩展的特征下手动计算点积了。但是有一个radial核函数，能够帮助我们获得这种能力。它的表达式如下：
$$
K(x, x^\prime) = e^{- {1\over2} \|x - x^\prime\|^2}
$$
当我们在高维特征空间对数据完成分类之后，再回到原始空间就能得到高维超平面在低维空间的投影，他们通常在低维空间是非线性的，如下图：

![highToLow](/assets/svm/highToLow.png)

### 总结

线性感知器模型提供了一个简单有效的二元分类框架。在它的基础上我们把学习问题转化为优化问题并怎加一些边界约束后，我们得到了支持向量机分类器，并通过hinge loss实现了软边界的特性。最后，再通过对特征空间的扩展，我们得到了在原始空间内的非线性分类的能力。最后，通过引用核函数来提高了分类器在高维特征空间下的计算效率。